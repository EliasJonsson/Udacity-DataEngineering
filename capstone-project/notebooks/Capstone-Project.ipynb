{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project a database was created of the immigration data in the United States from 2016. The database is structure with a star schema to simplify the querying.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import wikipedia\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from config import LocalConfigurations\n",
    "from schemas import demographics_schema, airport_schema, world_temp_schema, immigrations_schema\n",
    "\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, lit, split, isnull, to_timestamp, to_date\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType, DateType\n",
    "\n",
    "\n",
    "config = LocalConfigurations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "In this project the goal is to organize and combine four different datasets into one database. The database is organized to optimize for querying statistics of immigrations in the U.S. in the year 2016. The data will first all be stored in a raw format on AWS S3. The raw data will then be extracted from S3 with Apache Spark and transformed into 7 dimensional tables and 1 fact table following a star schema. A star schema was chosen both because of its simplicity and because of faster aggregations than many other data models. The database will again be stored in S3 using parquet files, but Amazon Athena can then be used to analyse the data further.\n",
    "\n",
    "#### Data\n",
    "- [I94 Immigration Data](https://www.trade.gov/national-travel-and-tourism-office) - Dataset that contains information on every immigration to the U.S. in 2016.\n",
    "- [World Temperature Data](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data?resource=download) - The temperature in different cities of the world collected on a daily basis.\n",
    "- [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) - Demographic data of cities in the U.S..\n",
    "- [Airport Code Table](https://datahub.io/core/airport-codes#data) - Data related to different airports in the world.\n",
    "\n",
    "![Source Data](../assets/udacity-data-sources.png \"Source Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://repos.spark-packages.org/ added as a remote repository with the name: repo-1\n",
      "Ivy Default Cache set to: /Users/elias/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/elias/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0d5eef13-b29c-406d-b78a-e97ae00c7a59;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;3.0.0-s_2.12 in spark-packages\n",
      "\tfound com.epam#parso;2.0.11 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/elias/Projects/Udacity-DataEngineering/capstone-project/venv/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      ":: resolution report :: resolve 263ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.11 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;3.0.0-s_2.12 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0d5eef13-b29c-406d-b78a-e97ae00c7a59\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 22:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    ".config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:3.0.0-s_2.12\") \\\n",
    ".config(\"spark.driver.memory\", \"15g\") \\\n",
    ".config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 22:21:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n",
      "-RECORD 0-------------------------------------------\n",
      " city                   | Silver Spring             \n",
      " state                  | Maryland                  \n",
      " median_age             | 33.8                      \n",
      " male_population        | 40601.0                   \n",
      " female_population      | 41862.0                   \n",
      " total_population       | 82463                     \n",
      " number_of_veterans     | 1562.0                    \n",
      " foreign_born           | 30908.0                   \n",
      " average_household_size | 2.6                       \n",
      " state_code             | MD                        \n",
      " race                   | Hispanic or Latino        \n",
      " count                  | 25924                     \n",
      "-RECORD 1-------------------------------------------\n",
      " city                   | Quincy                    \n",
      " state                  | Massachusetts             \n",
      " median_age             | 41.0                      \n",
      " male_population        | 44129.0                   \n",
      " female_population      | 49500.0                   \n",
      " total_population       | 93629                     \n",
      " number_of_veterans     | 4147.0                    \n",
      " foreign_born           | 32935.0                   \n",
      " average_household_size | 2.39                      \n",
      " state_code             | MA                        \n",
      " race                   | White                     \n",
      " count                  | 58723                     \n",
      "-RECORD 2-------------------------------------------\n",
      " city                   | Hoover                    \n",
      " state                  | Alabama                   \n",
      " median_age             | 38.5                      \n",
      " male_population        | 38040.0                   \n",
      " female_population      | 46799.0                   \n",
      " total_population       | 84839                     \n",
      " number_of_veterans     | 4819.0                    \n",
      " foreign_born           | 8229.0                    \n",
      " average_household_size | 2.58                      \n",
      " state_code             | AL                        \n",
      " race                   | Asian                     \n",
      " count                  | 4759                      \n",
      "-RECORD 3-------------------------------------------\n",
      " city                   | Rancho Cucamonga          \n",
      " state                  | California                \n",
      " median_age             | 34.5                      \n",
      " male_population        | 88127.0                   \n",
      " female_population      | 87105.0                   \n",
      " total_population       | 175232                    \n",
      " number_of_veterans     | 5821.0                    \n",
      " foreign_born           | 33878.0                   \n",
      " average_household_size | 3.18                      \n",
      " state_code             | CA                        \n",
      " race                   | Black or African-American \n",
      " count                  | 24437                     \n",
      "-RECORD 4-------------------------------------------\n",
      " city                   | Newark                    \n",
      " state                  | New Jersey                \n",
      " median_age             | 34.6                      \n",
      " male_population        | 138040.0                  \n",
      " female_population      | 143873.0                  \n",
      " total_population       | 281913                    \n",
      " number_of_veterans     | 5829.0                    \n",
      " foreign_born           | 86253.0                   \n",
      " average_household_size | 2.73                      \n",
      " state_code             | NJ                        \n",
      " race                   | White                     \n",
      " count                  | 76402                     \n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = true)\n",
      " |-- female_population: double (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: double (nullable = true)\n",
      " |-- foreign_born: double (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographic = spark.read.options(header='True', delimiter=';').csv(config.DEMOGRAPHICS_DATA_PATH, schema=demographics_schema)\n",
    "df_demographic.show(n=5, truncate=False, vertical=True)\n",
    "df_demographic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------\n",
      " ident        | 00A                                   \n",
      " type         | heliport                              \n",
      " name         | Total Rf Heliport                     \n",
      " elevation_ft | 11                                    \n",
      " continent    | NA                                    \n",
      " iso_country  | US                                    \n",
      " iso_region   | US-PA                                 \n",
      " municipality | Bensalem                              \n",
      " gps_code     | 00A                                   \n",
      " iata_code    | null                                  \n",
      " local_code   | 00A                                   \n",
      " coordinates  | -74.93360137939453, 40.07080078125    \n",
      "-RECORD 1---------------------------------------------\n",
      " ident        | 00AA                                  \n",
      " type         | small_airport                         \n",
      " name         | Aero B Ranch Airport                  \n",
      " elevation_ft | 3435                                  \n",
      " continent    | NA                                    \n",
      " iso_country  | US                                    \n",
      " iso_region   | US-KS                                 \n",
      " municipality | Leoti                                 \n",
      " gps_code     | 00AA                                  \n",
      " iata_code    | null                                  \n",
      " local_code   | 00AA                                  \n",
      " coordinates  | -101.473911, 38.704022                \n",
      "-RECORD 2---------------------------------------------\n",
      " ident        | 00AK                                  \n",
      " type         | small_airport                         \n",
      " name         | Lowell Field                          \n",
      " elevation_ft | 450                                   \n",
      " continent    | NA                                    \n",
      " iso_country  | US                                    \n",
      " iso_region   | US-AK                                 \n",
      " municipality | Anchor Point                          \n",
      " gps_code     | 00AK                                  \n",
      " iata_code    | null                                  \n",
      " local_code   | 00AK                                  \n",
      " coordinates  | -151.695999146, 59.94919968           \n",
      "-RECORD 3---------------------------------------------\n",
      " ident        | 00AL                                  \n",
      " type         | small_airport                         \n",
      " name         | Epps Airpark                          \n",
      " elevation_ft | 820                                   \n",
      " continent    | NA                                    \n",
      " iso_country  | US                                    \n",
      " iso_region   | US-AL                                 \n",
      " municipality | Harvest                               \n",
      " gps_code     | 00AL                                  \n",
      " iata_code    | null                                  \n",
      " local_code   | 00AL                                  \n",
      " coordinates  | -86.77030181884766, 34.86479949951172 \n",
      "-RECORD 4---------------------------------------------\n",
      " ident        | 00AR                                  \n",
      " type         | closed                                \n",
      " name         | Newport Hospital & Clinic Heliport    \n",
      " elevation_ft | 237                                   \n",
      " continent    | NA                                    \n",
      " iso_country  | US                                    \n",
      " iso_region   | US-AR                                 \n",
      " municipality | Newport                               \n",
      " gps_code     | null                                  \n",
      " iata_code    | null                                  \n",
      " local_code   | null                                  \n",
      " coordinates  | -91.254898, 35.6087                   \n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport = spark.read.options(header='True', delimiter=',').csv(config.AIRPORT_DATA_PATH, schema = airport_schema)\n",
    "df_airport.show(n=5, truncate=False, vertical=True)\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 22:21:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude\n",
      " Schema: dt, average_temperature, average_temperature_uncertainty, city, country, latitude, longitude\n",
      "Expected: average_temperature but found: AverageTemperature\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/GlobalLandTemperaturesByCity.csv\n",
      "-RECORD 0---------------------------------------------\n",
      " dt                              | 1743-11-01         \n",
      " average_temperature             | 6.068              \n",
      " average_temperature_uncertainty | 1.7369999999999999 \n",
      " city                            | Århus              \n",
      " country                         | Denmark            \n",
      " latitude                        | 57.05N             \n",
      " longitude                       | 10.33E             \n",
      "-RECORD 1---------------------------------------------\n",
      " dt                              | 1743-12-01         \n",
      " average_temperature             | null               \n",
      " average_temperature_uncertainty | null               \n",
      " city                            | Århus              \n",
      " country                         | Denmark            \n",
      " latitude                        | 57.05N             \n",
      " longitude                       | 10.33E             \n",
      "-RECORD 2---------------------------------------------\n",
      " dt                              | 1744-01-01         \n",
      " average_temperature             | null               \n",
      " average_temperature_uncertainty | null               \n",
      " city                            | Århus              \n",
      " country                         | Denmark            \n",
      " latitude                        | 57.05N             \n",
      " longitude                       | 10.33E             \n",
      "-RECORD 3---------------------------------------------\n",
      " dt                              | 1744-02-01         \n",
      " average_temperature             | null               \n",
      " average_temperature_uncertainty | null               \n",
      " city                            | Århus              \n",
      " country                         | Denmark            \n",
      " latitude                        | 57.05N             \n",
      " longitude                       | 10.33E             \n",
      "-RECORD 4---------------------------------------------\n",
      " dt                              | 1744-03-01         \n",
      " average_temperature             | null               \n",
      " average_temperature_uncertainty | null               \n",
      " city                            | Århus              \n",
      " country                         | Denmark            \n",
      " latitude                        | 57.05N             \n",
      " longitude                       | 10.33E             \n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      " |-- average_temperature_uncertainty: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature = spark.read.options(header='True', delimiter=',').csv(config.TEMPERATURE_DATA_PATH, schema = world_temp_schema)\n",
    "df_temperature.show(n=5, truncate=False, vertical=True)\n",
    "df_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(os.path.join(config.SAS_DATA_PATH, 'i94_apr16_sub.sas7bdat'), schema=immigrations_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#write to parquet\n",
    "folder = \"./sas_data\"\n",
    "\n",
    "if (os.path.exists(folder)):\n",
    "    shutil.rmtree(folder, ignore_errors=False, onerror=None)\n",
    "df_immigration.write.parquet(folder)\n",
    "df_immigration=spark.read.parquet(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- sas_arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- sas_depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "Number of entries are: 3096313\n",
      "-RECORD 0---------------------\n",
      " cic_id      | 5749531.0      \n",
      " i94yr       | 2016.0         \n",
      " i94mon      | 4.0            \n",
      " i94cit      | 251.0          \n",
      " i94res      | 251.0          \n",
      " i94port     | NYC            \n",
      " sas_arrdate | 20574.0        \n",
      " i94mode     | 1.0            \n",
      " i94addr     | NV             \n",
      " sas_depdate | 20579.0        \n",
      " i94bir      | 42.0           \n",
      " i94visa     | 1.0            \n",
      " count       | 1.0            \n",
      " dtadfile    | 20160430       \n",
      " visapost    | TLV            \n",
      " occup       | null           \n",
      " entdepa     | G              \n",
      " entdepd     | O              \n",
      " entdepu     | null           \n",
      " matflag     | M              \n",
      " biryear     | 1974.0         \n",
      " dtaddto     | 10292016       \n",
      " gender      | F              \n",
      " insnum      | null           \n",
      " airline     | DL             \n",
      " admnum      | 9.493188753E10 \n",
      " fltno       | 00469          \n",
      " visatype    | B1             \n",
      "-RECORD 1---------------------\n",
      " cic_id      | 5749532.0      \n",
      " i94yr       | 2016.0         \n",
      " i94mon      | 4.0            \n",
      " i94cit      | 251.0          \n",
      " i94res      | 251.0          \n",
      " i94port     | NYC            \n",
      " sas_arrdate | 20574.0        \n",
      " i94mode     | 1.0            \n",
      " i94addr     | NV             \n",
      " sas_depdate | 20584.0        \n",
      " i94bir      | 40.0           \n",
      " i94visa     | 2.0            \n",
      " count       | 1.0            \n",
      " dtadfile    | 20160430       \n",
      " visapost    | TLV            \n",
      " occup       | null           \n",
      " entdepa     | G              \n",
      " entdepd     | O              \n",
      " entdepu     | null           \n",
      " matflag     | M              \n",
      " biryear     | 1976.0         \n",
      " dtaddto     | 10292016       \n",
      " gender      | F              \n",
      " insnum      | null           \n",
      " airline     | DL             \n",
      " admnum      | 9.493207363E10 \n",
      " fltno       | 00469          \n",
      " visatype    | B2             \n",
      "-RECORD 2---------------------\n",
      " cic_id      | 5749533.0      \n",
      " i94yr       | 2016.0         \n",
      " i94mon      | 4.0            \n",
      " i94cit      | 251.0          \n",
      " i94res      | 251.0          \n",
      " i94port     | NYC            \n",
      " sas_arrdate | 20574.0        \n",
      " i94mode     | 1.0            \n",
      " i94addr     | NV             \n",
      " sas_depdate | 20584.0        \n",
      " i94bir      | 39.0           \n",
      " i94visa     | 2.0            \n",
      " count       | 1.0            \n",
      " dtadfile    | 20160430       \n",
      " visapost    | TLV            \n",
      " occup       | null           \n",
      " entdepa     | G              \n",
      " entdepd     | O              \n",
      " entdepu     | null           \n",
      " matflag     | M              \n",
      " biryear     | 1977.0         \n",
      " dtaddto     | 10292016       \n",
      " gender      | M              \n",
      " insnum      | null           \n",
      " airline     | DL             \n",
      " admnum      | 9.493197643E10 \n",
      " fltno       | 00469          \n",
      " visatype    | B2             \n",
      "-RECORD 3---------------------\n",
      " cic_id      | 5749538.0      \n",
      " i94yr       | 2016.0         \n",
      " i94mon      | 4.0            \n",
      " i94cit      | 251.0          \n",
      " i94res      | 251.0          \n",
      " i94port     | NYC            \n",
      " sas_arrdate | 20574.0        \n",
      " i94mode     | 1.0            \n",
      " i94addr     | NY             \n",
      " sas_depdate | null           \n",
      " i94bir      | 23.0           \n",
      " i94visa     | 2.0            \n",
      " count       | 1.0            \n",
      " dtadfile    | 20160430       \n",
      " visapost    | TLV            \n",
      " occup       | null           \n",
      " entdepa     | G              \n",
      " entdepd     | null           \n",
      " entdepu     | null           \n",
      " matflag     | null           \n",
      " biryear     | 1993.0         \n",
      " dtaddto     | 10292016       \n",
      " gender      | M              \n",
      " insnum      | null           \n",
      " airline     | EK             \n",
      " admnum      | 9.495134923E10 \n",
      " fltno       | 00203          \n",
      " visatype    | B2             \n",
      "-RECORD 4---------------------\n",
      " cic_id      | 5749540.0      \n",
      " i94yr       | 2016.0         \n",
      " i94mon      | 4.0            \n",
      " i94cit      | 251.0          \n",
      " i94res      | 251.0          \n",
      " i94port     | NYC            \n",
      " sas_arrdate | 20574.0        \n",
      " i94mode     | 1.0            \n",
      " i94addr     | NY             \n",
      " sas_depdate | 20575.0        \n",
      " i94bir      | 39.0           \n",
      " i94visa     | 2.0            \n",
      " count       | 1.0            \n",
      " dtadfile    | 20160430       \n",
      " visapost    | TLV            \n",
      " occup       | null           \n",
      " entdepa     | G              \n",
      " entdepd     | I              \n",
      " entdepu     | null           \n",
      " matflag     | M              \n",
      " biryear     | 1977.0         \n",
      " dtaddto     | 10292016       \n",
      " gender      | M              \n",
      " insnum      | null           \n",
      " airline     | PR             \n",
      " admnum      | 9.502525553E10 \n",
      " fltno       | 00126          \n",
      " visatype    | B2             \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.printSchema()\n",
    "print(f\"Number of entries are: {df_immigration.count()}\")\n",
    "df_immigration.show(n=5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the **Data Exploration Notebook** for the data explorations steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_sas_description\n",
    "sas_data = read_sas_description(config.SAS_DESC_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_ref_date = datetime(1960, 1, 1)\n",
    "\n",
    "@udf\n",
    "def to_city(id):\n",
    "    try:\n",
    "        return sas_data['I94CIT'][int(id)]\n",
    "    except:\n",
    "        return \"No Country Code\"\n",
    "\n",
    "@udf\n",
    "def to_port(port_id):\n",
    "    try:\n",
    "        return sas_data['I94PORT'][port_id]\n",
    "    except:\n",
    "        return \"No PORT Code\"\n",
    "\n",
    "@udf\n",
    "def to_travel_mode(mode_id):\n",
    "    try:\n",
    "        return sas_data['I94MODE'][int(mode_id)]\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "@udf\n",
    "def to_transport_no(transport_no, company_name, mode):\n",
    "    if transport_no:\n",
    "        return str(transport_no)\n",
    "    no = 'no'\n",
    "    if company_name:\n",
    "        return no\n",
    "    if company_name:\n",
    "        no += str(company_name[0:3])\n",
    "    if mode:\n",
    "        no += str(mode)\n",
    "    return no\n",
    "\n",
    "@udf\n",
    "def to_visa_type(visa_id):\n",
    "    try:\n",
    "        return sas_data['I94VISA'][visa_id]\n",
    "    except:\n",
    "        None\n",
    "\n",
    "@udf\n",
    "def to_int(number):\n",
    "    if number:\n",
    "        try:\n",
    "            return int(number)\n",
    "        except:\n",
    "            print(number)\n",
    "            return 0\n",
    "    return None\n",
    "\n",
    "@udf(TimestampType())\n",
    "def to_timestamp_sas(days):\n",
    "    try:\n",
    "        return sas_ref_date + timedelta(days=int(days))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "@udf(DateType())\n",
    "def to_date_sas(days):\n",
    "    try:\n",
    "        return sas_ref_date + timedelta(days=int(days))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "@udf(StringType())\n",
    "def upper(val):\n",
    "    return val.upper()\n",
    "\n",
    "@udf(StringType())\n",
    "def city_sas(val):\n",
    "    return sas_data['I94PORT'][val][0]\n",
    "\n",
    "@udf(StringType())\n",
    "def state_sas(val):\n",
    "    return sas_data['I94PORT'][val][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Data Model\n",
    "#### Conceptual Data Model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project it was decided to use star schema to model the data. The star schema was chosen for fast and simple way of analysing the data.\n",
    "\n",
    "![Conceptual Data model](../assets/udacity-star-schema.png \"Conceptual Data Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipelines to Model the Data \n",
    "#### Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cic_id=5749531.0, i94yr=2016.0, i94mon=4.0, i94cit=251.0, i94res=251.0, i94port='NYC', sas_arrdate=20574.0, i94mode=1.0, i94addr='NV', sas_depdate=20579.0, i94bir=42.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='TLV', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1974.0, dtaddto='10292016', gender='F', insnum=None, airline='DL', admnum=94931887530.0, fltno='00469', visatype='B1', arrdate=datetime.date(2016, 4, 30), depdate=datetime.date(2016, 5, 5), expiration_date=datetime.date(2016, 10, 29), transport_no='00469', city='NEW YORK', state='NY')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Augment data\n",
    "df_immigration_aug = df_immigration \\\n",
    "    .withColumn('arrdate', to_date_sas('sas_arrdate')) \\\n",
    "    .withColumn('depdate', to_date_sas('sas_depdate')) \\\n",
    "    .withColumn('expiration_date', to_date('dtaddto', \"MMddyyyy\")) \\\n",
    "    .withColumn('transport_no',  to_transport_no('fltno', 'airline', 'i94mode')) \\\n",
    "    .withColumn('city', city_sas(df_immigration.i94port)) \\\n",
    "    .withColumn('state', state_sas(df_immigration.i94port)) \\\n",
    "    .na.drop(subset=[\"admnum\"])\n",
    "df_immigration_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = false)\n",
      " |-- female_population: double (nullable = false)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: double (nullable = false)\n",
      " |-- foreign_born: double (nullable = false)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = true)\n",
      " |-- female_population: double (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: double (nullable = true)\n",
      " |-- foreign_born: double (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- most_common_race: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- city_id: long (nullable = false)\n",
      "\n",
      "23/02/28 23:32:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n",
      "23/02/28 23:32:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: AverageTemperature, City, Country\n",
      " Schema: average_temperature, city, country\n",
      "Expected: average_temperature but found: AverageTemperature\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/GlobalLandTemperaturesByCity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 170:===============================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.8 ms, sys: 15.1 ms, total: 59.8 ms\n",
      "Wall time: 6.64 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(state_code='AK', name='ALCAN', state=None, median_age=None, male_population=None, female_population=None, total_population=None, number_of_veterans=None, foreign_born=None, average_household_size=None, most_common_race=None, country=None, avg_temp=None, city_id=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# city_id, name, state, median_age, male_population, female_population, total_population,\n",
    "# number_of_veterans, foreign_born, average_household_size, state_code, most_common_race, country, avg_temp\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "# 1. Cleaning demographic data\n",
    "#      1.1 Fill NaNs and remove duplicated rows if exists\n",
    "col_names = ['male_population', 'female_population', 'number_of_veterans', 'foreign_born']\n",
    "df_demographic_structured = df_demographic \\\n",
    "    .na.fill(value=0, subset=col_names) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "df_demographic_structured.printSchema()\n",
    "\n",
    "# 1. Cleaning temperature data\n",
    "#      2.1 Average Temperature in the US\n",
    "df_temperature_structured = df_temperature \\\n",
    "    .select('city', 'country', 'average_temperature') \\\n",
    "    .where(df_temperature.country == 'United States') \\\n",
    "    .groupBy('city', 'country').mean() \\\n",
    "    .withColumnRenamed('avg(average_temperature)', 'avg_temp') \\\n",
    "    .drop('country')\n",
    "\n",
    "\n",
    "# 3. Reconstruction\n",
    "#      3.1 - Find most common race for each city-state pair.\n",
    "df_demographic_structured.createOrReplaceTempView(\"demographic_race_count\")\n",
    "df_demographic_count = spark.sql(\"select city, state, count from demographic_race_count\") \\\n",
    "    .groupby('city', 'state') \\\n",
    "    .max() \\\n",
    "    .withColumnRenamed('max(count)', 'count')\n",
    "df_demographic_structured = df_demographic_structured \\\n",
    "    .join(df_demographic_count, on=['city', 'state', 'count'], how='inner') \\\n",
    "    .withColumnRenamed('race', 'most_common_race') \\\n",
    "    .withColumnRenamed('city', 'name') \\\n",
    "    .withColumn('name', upper(col('name'))) \\\n",
    "    .withColumn('country', lit('US')) \\\n",
    "    .drop('count')\n",
    "\n",
    "df_demographic_temp = df_demographic_structured \\\n",
    "    .join(df_temperature_structured,df_demographic_structured.name ==  df_temperature_structured.city,\"left\") \\\n",
    "    .drop('city')\n",
    "\n",
    "city_table = df_demographic_temp \\\n",
    "        .join(df_immigration_aug.select(col('state').alias('state_code'), col('city').alias('name')).dropDuplicates(), ['state_code', 'name'], \"fullouter\") \\\n",
    "        .withColumn('city_id', monotonically_increasing_id())\n",
    "city_table.printSchema()\n",
    "city_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Port Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- continent: string (nullable = false)\n",
      " |-- type: string (nullable = true)\n",
      " |-- port_id: long (nullable = false)\n",
      "\n",
      "CPU times: user 16.1 ms, sys: 6.61 ms, total: 22.7 ms\n",
      "Wall time: 621 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(port='00A', name='Total Rf Heliport', municipality='Bensalem', country='US', elevation_ft=11, longitude='-74.93360137939453', latitude=' 40.07080078125', continent='NA', type='heliport', port_id=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1. Cleaning ports\n",
    "# port_id, port_code, name, city, country, elevation_ft, longitude, latitude\n",
    "#      1.1 Only U.S. airports\n",
    "port_table = df_airport \\\n",
    "    .join(df_immigration_aug.select(col('i94port').alias('local_code')).dropDuplicates(), ['local_code'], \"fullouter\") \\\n",
    "    .select('local_code', 'name', 'municipality', 'iso_country', 'elevation_ft'\n",
    "            , split('coordinates', ',').getItem(0).alias(\"longitude\")\n",
    "            , split('coordinates', ',').getItem(1).alias(\"latitude\")\n",
    "            , lit('NA').alias('continent')\n",
    "            , 'type') \\\n",
    "    .where(df_airport['iso_country'] == \"US\") \\\n",
    "    .withColumnRenamed('local_code', 'port') \\\n",
    "    .withColumnRenamed('city', 'municipality') \\\n",
    "    .withColumnRenamed('iso_country', 'country') \\\n",
    "    .withColumn('port_id', monotonically_increasing_id())\n",
    "port_table.printSchema()\n",
    "port_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alien Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- admission_id: string (nullable = true)\n",
      " |-- citizenship_origin: string (nullable = true)\n",
      " |-- residency_origin: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- year_of_birth: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 185:===============================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.5 ms, sys: 13 ms, total: 78.5 ms\n",
      "Wall time: 16.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(admission_id='1023352085', citizenship_origin='MEXICO Air Sea, and Not Reported (I-94, no land arrivals)', residency_origin='MEXICO Air Sea, and Not Reported (I-94, no land arrivals)', gender='F', age='16', visatype='E2', i94visa='Business', entdepa='T', entdepd='O', matflag='M', year_of_birth='2000')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1.1 Create alien table\n",
    "alien_table = df_immigration_aug.select(\n",
    "    to_int(df_immigration_aug.admnum).alias('admission_id'),\n",
    "    to_city(df_immigration.i94cit).alias('citizenship_origin'),\n",
    "    to_city(df_immigration.i94res).alias('residency_origin'),\n",
    "    df_immigration.gender,\n",
    "    to_int(df_immigration.i94bir).alias('age'),\n",
    "    df_immigration.visatype,\n",
    "    to_visa_type(df_immigration.i94visa).alias('i94visa'),\n",
    "    df_immigration.entdepa,\n",
    "    df_immigration.entdepd,\n",
    "    df_immigration.matflag,\n",
    "    to_int(df_immigration.biryear).alias('year_of_birth'),\n",
    ").dropDuplicates(['admission_id']) \\\n",
    ".na.drop(subset=[\"admission_id\"])\n",
    "alien_table.printSchema()\n",
    "alien_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 189:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 ms, sys: 8.34 ms, total: 34.1 ms\n",
      "Wall time: 1.89 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(time=datetime.datetime(2016, 4, 30, 0, 0), hour=0, day=30, week=17, month=4, year=2016, weekday=7)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract columns to create time table\n",
    "arr_time_table = df_immigration_aug.select(\n",
    "    to_timestamp_sas(df_immigration_aug.sas_arrdate).alias('time'),\n",
    "    hour(df_immigration_aug.arrdate).alias('hour'),\n",
    "    dayofmonth(df_immigration_aug.arrdate).alias('day'),\n",
    "    weekofyear(df_immigration_aug.arrdate).alias('week'),\n",
    "    month(df_immigration_aug.arrdate).alias('month'),\n",
    "    year(df_immigration_aug.arrdate).alias('year'),\n",
    "    dayofweek(df_immigration_aug.arrdate).alias('weekday')\n",
    ")\n",
    "dep_time_table = df_immigration_aug.select(\n",
    "    to_timestamp_sas(df_immigration_aug.sas_depdate).alias('time'),\n",
    "    hour(df_immigration_aug.depdate).alias('hour'),\n",
    "    dayofmonth(df_immigration_aug.depdate).alias('day'),\n",
    "    weekofyear(df_immigration_aug.depdate).alias('week'),\n",
    "    month(df_immigration_aug.depdate).alias('month'),\n",
    "    year(df_immigration_aug.depdate).alias('year'),\n",
    "    dayofweek(df_immigration_aug.depdate).alias('weekday')\n",
    ")\n",
    "\n",
    "expiration_time_table = df_immigration_aug.select(\n",
    "    to_timestamp(df_immigration_aug.expiration_date).alias('time'),\n",
    "    hour(df_immigration_aug.expiration_date).alias('hour'),\n",
    "    dayofmonth(df_immigration_aug.expiration_date).alias('day'),\n",
    "    weekofyear(df_immigration_aug.expiration_date).alias('week'),\n",
    "    month(df_immigration_aug.expiration_date).alias('month'),\n",
    "    year(df_immigration_aug.expiration_date).alias('year'),\n",
    "    dayofweek(df_immigration_aug.expiration_date).alias('weekday')\n",
    ")\n",
    "expiration_time_table.head()\n",
    "time_table = arr_time_table \\\n",
    "    .union(dep_time_table) \\\n",
    "    .union(expiration_time_table) \\\n",
    "    .dropDuplicates() \\\n",
    "    .na.drop()\n",
    "time_table.printSchema()\n",
    "time_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visa Issuer Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- visa_issuer_id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 190:====>         (12 + 12) / 42][Stage 191:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 23:34:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unifi:59504\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/02/28 23:34:48 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unifi:59504\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:==========>   (30 + 12) / 42][Stage 191:>                (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37 ms, sys: 15 ms, total: 52 ms\n",
      "Wall time: 57.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(name='TLV', visa_issuer_id=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "visa_issuer_table = df_immigration.select(\n",
    "    df_immigration.visapost.alias('name'),\n",
    ").dropDuplicates()\n",
    "visa_issuer_table = visa_issuer_table.withColumn(\"visa_issuer_id\", monotonically_increasing_id())\n",
    "\n",
    "visa_issuer_table.printSchema()\n",
    "visa_issuer_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transport_no: string (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- transport_id: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:=============> (38 + 4) / 42][Stage 192:=============> (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.5 ms, sys: 11.1 ms, total: 41.5 ms\n",
      "Wall time: 8.21 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 190:=================================================>     (38 + 4) / 42]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(transport_no='00000', company='JL', mode=1.0, transport_id=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logistic_table = df_immigration_aug.select(\n",
    "    df_immigration_aug.transport_no,\n",
    "    df_immigration_aug.airline.alias('company'),\n",
    "    df_immigration_aug.i94mode.alias('mode'),\n",
    "    \n",
    ") \\\n",
    ".dropDuplicates(['transport_no']) \\\n",
    ".withColumn(\"transport_id\", monotonically_increasing_id())\n",
    "\n",
    "logistic_table.printSchema()\n",
    "logistic_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: double (nullable = true)\n",
      " |-- arrival_city: long (nullable = true)\n",
      " |-- expiration_time: timestamp (nullable = true)\n",
      " |-- admission_id: string (nullable = true)\n",
      " |-- arrdate: timestamp (nullable = true)\n",
      " |-- depdate: timestamp (nullable = true)\n",
      " |-- transport_id: long (nullable = true)\n",
      " |-- visa_issuer_id: long (nullable = true)\n",
      " |-- i94_year: double (nullable = true)\n",
      " |-- i94_month: double (nullable = true)\n",
      "\n",
      "23/02/28 23:35:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.5 ms, sys: 19.4 ms, total: 74.9 ms\n",
      "Wall time: 15.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cic_id=878805.0, arrival_city=43, expiration_time=None, admission_id='92770784430', arrdate=datetime.datetime(2016, 4, 5, 0, 0), depdate=datetime.datetime(2016, 6, 8, 0, 0), transport_id=1679, visa_issuer_id=67, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=6.0, arrival_city=532, expiration_time=None, admission_id='1897628485', arrdate=datetime.datetime(2016, 4, 29, 0, 0), depdate=None, transport_id=2112, visa_issuer_id=None, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=15.0, arrival_city=762, expiration_time=None, admission_id='666643185', arrdate=datetime.datetime(2016, 4, 1, 0, 0), depdate=datetime.datetime(2016, 8, 25, 0, 0), transport_id=1386, visa_issuer_id=None, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904933.0, arrival_city=762, expiration_time=None, admission_id='94622009230', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 5, 24, 0, 0), transport_id=2873, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904934.0, arrival_city=762, expiration_time=None, admission_id='94592305630', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 6, 2, 0, 0), transport_id=2617, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904935.0, arrival_city=762, expiration_time=None, admission_id='94592424830', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 6, 2, 0, 0), transport_id=2617, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904936.0, arrival_city=762, expiration_time=None, admission_id='94621172330', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 6, 23, 0, 0), transport_id=2873, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904937.0, arrival_city=762, expiration_time=None, admission_id='94621347430', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 6, 23, 0, 0), transport_id=2873, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904938.0, arrival_city=762, expiration_time=None, admission_id='94620804830', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 6, 28, 0, 0), transport_id=2873, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0),\n",
       " Row(cic_id=4904939.0, arrival_city=762, expiration_time=None, admission_id='94620865230', arrdate=datetime.datetime(2016, 4, 26, 0, 0), depdate=datetime.datetime(2016, 6, 28, 0, 0), transport_id=2873, visa_issuer_id=195, i94_year=2016.0, i94_month=4.0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 23:36:28 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unifi:59504\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/02/28 23:36:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unifi:59504\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 23:38:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unifi:59504\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "23/02/28 23:38:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:643)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1057)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:593)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:592)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:630)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@unifi:59504\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "immigration_fact_table = df_immigration_aug \\\n",
    "    .join(city_table, ((df_immigration_aug.city == city_table.name) & (df_immigration_aug.state == city_table.state_code)), \"left\") \\\n",
    "    .join(port_table, (df_immigration_aug.i94port == port_table.port), \"left\") \\\n",
    "    .join(visa_issuer_table, (df_immigration_aug.visapost == visa_issuer_table.name), \"left\") \\\n",
    "    .join(logistic_table, (df_immigration_aug.transport_no == logistic_table.transport_no), \"left\") \\\n",
    "    .select(\n",
    "        col('cic_id'),\n",
    "        col('city_id').alias('arrival_city'),\n",
    "        to_timestamp(col('dtaddto')).alias('expiration_time'),\n",
    "        to_int(col('admnum')).alias('admission_id'),\n",
    "        to_timestamp_sas(col('sas_arrdate')).alias('arrdate'),\n",
    "        to_timestamp_sas(col('sas_depdate')).alias('depdate'),\n",
    "        col('transport_id'),\n",
    "        col('visa_issuer_id'),\n",
    "        col('i94Yr').alias('i94_year'),\n",
    "        col('i94Mon').alias('i94_month'),\n",
    "    )\n",
    "\n",
    "immigration_fact_table.printSchema()\n",
    "immigration_fact_table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checks\n",
    "The quality checks are pretty simple but the checks fail if there is a null value in a primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 20:30:18 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "def raise_if_null(table, col_name):\n",
    "    if table.filter(col(col_name).isNull()).count() > 0:\n",
    "        raise ValueError(f\"The column {col_name} in table {table} had a NULL value!\")\n",
    "\n",
    "raise_if_null(immigration_fact_table, 'cic_id')\n",
    "raise_if_null(logistic_table, 'transport_id')\n",
    "raise_if_null(visa_issuer_table, 'visa_issuer_id')\n",
    "raise_if_null(time_table, 'time')\n",
    "raise_if_null(alien_table, 'admission_id')\n",
    "raise_if_null(city_table, 'city_id')\n",
    "raise_if_null(port_table, 'port_id')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data dictionary](../assets/udacity-data-dictionary.png \"Data dictionary\") \n",
    "\n",
    "[I94 Immigration Data](https://www.trade.gov/national-travel-and-tourism-office): This data was extracted from the US National Tourism and Trade Office.\n",
    "\n",
    "[World Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data): This dataset is from Kaggle an online community platform for data scientists and machine learning enthusiasts.\n",
    "\n",
    "[U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/): Comes from OpenSoft.\n",
    "\n",
    "[Airport Code Table](https://datahub.io/core/airport-codes#data): Extracted from datahub.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, a ETL process is implemented where the data is extracted from S3, transformed to a star schema using Pyspark on an EMR cluster, and then loaded back to S3\n",
    "\n",
    "S3 is used as a data source because it is easy to use, the performance is far better than needed for this project and the cost is reasonable. In this project, a significant amount of data is being used and therefore it can take hours to days for a single computer to transform the data. To speed up the processing time a network of computers is used to process the data. [Amazon EMR](https://aws.amazon.com/emr/) pre-configured with [Apache Spark](https://spark.apache.org/) was used as the tool for the ETL process, but EMR is very good for managing the cluster and Spark for doing distributed processing.\n",
    "\n",
    "Python 3.7 was chosen as the programming language, and therefore Pyspark as well, but python is really good for fast development and for learning new things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of updates on this database should be on approximately monthly basis, but this tool is mostly built for people interested in analysing immigration data for historical reasons but not for user needing live updates.\n",
    "\n",
    "If the data was increased by 100x more resources and more powerful instances should be used. This could all be set in the `../../Exercices/EMRSetup/create-cluster.sh` with the flags `instance-count` and `instance-type`.\n",
    "\n",
    "If the data had to be updated on a daily basis by 7am it would be very good to use a scheduling tool like [Apache Airflow](https://airflow.apache.org/).\n",
    "\n",
    "If the database needed to be accessed by 100+ people it would be good to load the data from S3 to Amazon Redshift, but with Amazon Redshift it is easy to do SQL queries that most business analytics should be familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 10 most common arrival state for immigrants \n",
    "most_common_arrival_state = immigration_fact_table.select('arrival_city').join(city_table.dropna(subset=[\"State\"]), city_table.city_id == immigration_fact_table.arrival_city, \"inner\") \\\n",
    "    .groupby([\"State\"]) \\\n",
    "    .count().alias(\"count\") \\\n",
    "    .sort(\"count\", inplace=True, ascending=False) \\\n",
    "    .select(\"State\", \"count\") \\\n",
    "    .limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 21:13:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(State='Florida', count=641466), Row(State='California', count=487174), Row(State='New York', count=487108), Row(State='Texas', count=185047), Row(State='Illinois', count=130564), Row(State='Georgia', count=92608), Row(State='Nevada', count=89281), Row(State='Massachusetts', count=57354), Row(State='Washington', count=47726), Row(State='Arizona', count=39109)]\n",
      "23/02/28 21:13:19 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(most_common_arrival_state.head(10))\n",
    "most_common_arrival_state.write.format('csv').option('header','true').save(os.path.join(config.OUTPUT_DATA, 'arrival_state_count_small'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common arrival months for immigrants \n",
    "most_common_months = immigration_fact_table.select('arrdate').join(time_table.select('time', 'month'), time_table.time == immigration_fact_table.arrdate, \"inner\") \\\n",
    "    .groupby([\"month\"]) \\\n",
    "    .count().alias(\"count\") \\\n",
    "    .sort(\"count\", inplace=True, ascending=False) \\\n",
    "    .select(\"month\", \"count\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 242:>(2 + 12) / 14][Stage 243:>  (0 + 0) / 1][Stage 244:>  (0 + 0) / 1]4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 21:25:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\n",
      "\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\n",
      "\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\n",
      "\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n",
      "[Stage 289:>(0 + 12) / 14][Stage 290:>  (0 + 0) / 1][Stage 291:>  (0 + 0) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 21:32:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_common_months.head(12)\n",
    "most_common_months.write.format('csv').option('header','true').save(os.path.join(config.OUTPUT_DATA, 'most_common_arrival_months_small'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 347:>(2 + 12) / 14][Stage 348:>  (0 + 0) / 1][Stage 349:>  (0 + 0) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 21:43:17 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count\n",
      " Schema: city, state, median_age, male_population, female_population, total_population, number_of_veterans, foreign_born, average_household_size, state_code, race, count\n",
      "Expected: median_age but found: Median Age\n",
      "CSV file: file:///Users/elias/Projects/Udacity-DataEngineering/capstone-project/data/us-cities-demographics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find most common arrival months for Icelanders\n",
    "immigrants_from_iceland_per_month = immigration_fact_table.select('admission_id', 'arrdate') \\\n",
    "    .join(time_table.select('time', 'month'), time_table.time == immigration_fact_table.arrdate, \"inner\") \\\n",
    "    .join(alien_table.select('admission_id', 'citizenship_origin'), alien_table.admission_id == immigration_fact_table.admission_id, \"inner\") \\\n",
    "    .where(col('citizenship_origin') == 'ICELAND') \\\n",
    "    .groupby('month') \\\n",
    "    .count().alias(\"count\") \\\n",
    "    .select('month', \"count\")\n",
    "immigrants_from_iceland_per_month.write.format('csv').option('header','true').save(os.path.join(config.OUTPUT_DATA, 'icelander_immigrant_months_count_small'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
